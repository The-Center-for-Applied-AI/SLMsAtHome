# SLMs@Home  
An easy-to-run crowdsourced benchmarking application  

## About  
SLMs@Home is an open-source benchmarking tool designed to allow users to evaluate and compare Small Language Models (SLMs) in a decentralized manner. This project is released under the Apache License 2.0 and is maintained by the Center for Applied AI (C4AI).  

## How This Differs from the Version on [benchmarks.c4ai.ai](benchmarks.c4ai.ai)
This repository contains a **stripped-down open-source version** of SLMs@Home, based on an internal version developed at C4AI.  

The version available on [benchmarks.c4ai.ai](benchmarks.c4ai.ai) is a separately maintained internal fork with additional features, including:  
- A networking layer to interface with the leaderboard  
- A security stack to prevent spoofed results  
- Upload limitations to prevent overwhelming the server  

These modifications are not included in this open-source version, which is designed for independent benchmarking.  

## License  
This project is licensed under the Apache License 2.0. See the LICENSE file for details.  

By contributing to this project, you agree that your contributions will be licensed under the same terms.   

## Contributions  
We welcome contributions from the community! If you'd like to suggest improvements or add new features, feel free to open an issue or submit a pull request.  

## Disclaimer  
This open-source version of SLMs@Home is a **stripped-down** version of an internal benchmarking tool developed at C4AI. It does not include proprietary or server-side components from the internal version hosted at [benchmarks.c4ai.ai](benchmarks.c4ai.ai).  
